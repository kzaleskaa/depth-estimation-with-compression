{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quantization steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "from tinynn.graph.quantization.quantizer import QATQuantizer\n",
    "from torchvision.transforms import transforms\n",
    "from src.models.unet_module import UNETLitModule\n",
    "from src.models.components.depth_net_efficient_ffn import DepthNet\n",
    "from src.data.components.nyu_dataset import NYUDataset\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from src.data.components.custom_transforms import BilinearInterpolation, NormalizeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"logs/train/runs/2024-05-31_03-13-02/checkpoints/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/tscratch/people/plgkzaleska/envs/ennca-project/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'net' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['net'])`.\n"
     ]
    }
   ],
   "source": [
    "model = UNETLitModule.load_from_checkpoint(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DepthNet(\n",
       "  (encoder): EfficientNet(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UpHeadA(\n",
       "    (layers): Sequential(\n",
       "      (0): AdapterConv(\n",
       "        (adapter_conv): ModuleList(\n",
       "          (0): ConvBNReLU(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(24, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBNReLU(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(40, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): ConvBNReLU(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(80, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): UpBranch(\n",
       "        (fam_32_sm): ConvBNReLU(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (fam_32_up): ConvBNReLU(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (fam_16_sm): ConvBNReLU(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (fam_16_up): ConvBNReLU(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (fam_8_sm): ConvBNReLU(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpsampleCat()\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv2d(384, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fuse BatchNorm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fuse = quantize_fx.fuse_fx(model.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (net): Module(\n",
       "    (encoder): Module(\n",
       "      (layer1): Module(\n",
       "        (0): Module(\n",
       "          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): Module(\n",
       "          (0): Module(\n",
       "            (block): Module(\n",
       "              (0): Module(\n",
       "                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Module(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (scale_activation): Sigmoid()\n",
       "              )\n",
       "              (2): Module(\n",
       "                (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (0): Module(\n",
       "            (block): Module(\n",
       "              (0): Module(\n",
       "                (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Module(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "              (2): Module(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (scale_activation): Sigmoid()\n",
       "              )\n",
       "              (3): Module(\n",
       "                (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): Module(\n",
       "            (block): Module(\n",
       "              (0): Module(\n",
       "                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Module(\n",
       "                (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "              (2): Module(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): SiLU(inplace=True)\n",
       "                (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (scale_activation): Sigmoid()\n",
       "              )\n",
       "              (3): Module(\n",
       "                (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer2): Module(\n",
       "        (0): Module(\n",
       "          (block): Module(\n",
       "            (0): Module(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Module(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Module(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Module(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): Module(\n",
       "            (0): Module(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Module(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Module(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Module(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer3): Module(\n",
       "        (0): Module(\n",
       "          (block): Module(\n",
       "            (0): Module(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Module(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Module(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Module(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): Module(\n",
       "            (0): Module(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Module(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Module(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Module(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): Module(\n",
       "            (0): Module(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Module(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Module(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Module(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): Module(\n",
       "      (layers): Module(\n",
       "        (0): Module(\n",
       "          (adapter_conv): Module(\n",
       "            (0): Module(\n",
       "              (layers): Module(\n",
       "                (0): ConvReLU2d(\n",
       "                  (0): Conv2d(24, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (1): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): Module(\n",
       "              (layers): Module(\n",
       "                (0): ConvReLU2d(\n",
       "                  (0): Conv2d(40, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (1): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): Module(\n",
       "              (layers): Module(\n",
       "                (0): ConvReLU2d(\n",
       "                  (0): Conv2d(80, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (1): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Module(\n",
       "          (fam_32_sm): Module(\n",
       "            (layers): Module(\n",
       "              (0): ConvReLU2d(\n",
       "                (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (1): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (fam_32_up): Module(\n",
       "            (layers): Module(\n",
       "              (0): ConvReLU2d(\n",
       "                (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (fam_16_sm): Module(\n",
       "            (layers): Module(\n",
       "              (0): ConvReLU2d(\n",
       "                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (1): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (fam_16_up): Module(\n",
       "            (layers): Module(\n",
       "              (0): ConvReLU2d(\n",
       "                (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (1): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (fam_8_sm): Module(\n",
       "            (layers): Module(\n",
       "              (0): ConvReLU2d(\n",
       "                (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (1): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_conv): Conv2d(384, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PTQ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNETLitModule(\n",
       "  (net): DepthNet(\n",
       "    (encoder): EfficientNet(\n",
       "      (layer1): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): SiLU(inplace=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): MBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): SiLU(inplace=True)\n",
       "                (scale_activation): Sigmoid()\n",
       "              )\n",
       "              (2): Conv2dNormActivation(\n",
       "                (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): MBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): SiLU(inplace=True)\n",
       "                (scale_activation): Sigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
       "          )\n",
       "          (1): MBConv(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "              (1): Conv2dNormActivation(\n",
       "                (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "                (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): SiLU(inplace=True)\n",
       "              )\n",
       "              (2): SqueezeExcitation(\n",
       "                (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (activation): SiLU(inplace=True)\n",
       "                (scale_activation): Sigmoid()\n",
       "              )\n",
       "              (3): Conv2dNormActivation(\n",
       "                (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): UpHeadA(\n",
       "      (layers): Sequential(\n",
       "        (0): AdapterConv(\n",
       "          (adapter_conv): ModuleList(\n",
       "            (0): ConvBNReLU(\n",
       "              (layers): Sequential(\n",
       "                (0): Conv2d(24, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (1): ConvBNReLU(\n",
       "              (layers): Sequential(\n",
       "                (0): Conv2d(40, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (2): ConvBNReLU(\n",
       "              (layers): Sequential(\n",
       "                (0): Conv2d(80, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): UpBranch(\n",
       "          (fam_32_sm): ConvBNReLU(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (fam_32_up): ConvBNReLU(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (fam_16_sm): ConvBNReLU(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (fam_16_up): ConvBNReLU(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (fam_8_sm): ConvBNReLU(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): UpsampleCat()\n",
       "      )\n",
       "    )\n",
       "    (final_conv): Conv2d(384, 1, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       "  (train_ssim): StructuralSimilarityIndexMeasure()\n",
       "  (val_ssim): StructuralSimilarityIndexMeasure()\n",
       "  (test_ssim): StructuralSimilarityIndexMeasure()\n",
       "  (train_loss): MeanMetric()\n",
       "  (val_loss): MeanMetric()\n",
       "  (test_loss): MeanMetric()\n",
       "  (val_ssim_best): MaxMetric()\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_img = transforms.Compose(\n",
    "    [transforms.PILToTensor(), transforms.Resize((224, 224))]\n",
    ")\n",
    "\n",
    "transforms_mask_train = transforms.Compose(\n",
    "    [transforms.ToTensor(), BilinearInterpolation((56, 56))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NYUDataset(\n",
    "    \"nyu2_train.csv\",\n",
    "    \"data/\",\n",
    "    transform=transforms_img,\n",
    "    target_transform=transforms_mask_train\n",
    ")\n",
    "\n",
    "data_train, data_val = random_split(\n",
    "    dataset=trainset,\n",
    "    lengths=[0.8, 0.2],\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=data_val,\n",
    "    batch_size=32,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration(model, num_iterations, val_dataloader):\n",
    "    count = 0\n",
    "    for data in val_dataloader:\n",
    "        img, mask = data\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "            mask = mask.cuda()\n",
    "        model(img)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if count >= num_iterations:\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer_per_tensor = QATQuantizer(\n",
    "    model,\n",
    "    torch.randn(1,3,52,52),\n",
    "    work_dir='quant_output',\n",
    "    config={\n",
    "        'asymmetric': True,\n",
    "        'backend': 'qnnpack',\n",
    "        \"disable_requantization_for_cat\": True,\n",
    "        'per_tensor': True,\n",
    "})\n",
    "\n",
    "quantizer_per_channel = QATQuantizer(\n",
    "    model,\n",
    "    torch.randn(1,3,52,52),\n",
    "    work_dir='quant_output',\n",
    "    config={\n",
    "        'asymmetric': True,\n",
    "        'backend': 'qnnpack',\n",
    "        \"disable_requantization_for_cat\": True,\n",
    "        'per_tensor': False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptq_model_with_quantizer_tensor = quantizer_per_tensor.quantize()\n",
    "ptq_model_with_quantizer_channel = quantizer_per_channel.quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QUNETLitModule(\n",
       "  (fake_quant_0): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_0): ConvBn2d(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_1): Identity()\n",
       "  (net_encoder_layer1_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_0): ConvBn2d(\n",
       "    32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_1_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_1_0_block_1_fc1): Conv2d(\n",
       "    32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_fc2): Conv2d(\n",
       "    8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_0): ConvBn2d(\n",
       "    32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_0): ConvBn2d(\n",
       "    16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_0): ConvBn2d(\n",
       "    96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_0_block_2_fc1): Conv2d(\n",
       "    96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_fc2): Conv2d(\n",
       "    4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_0): ConvBn2d(\n",
       "    96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_1_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_0): ConvBn2d(\n",
       "    144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "  (net_encoder_layer2_0_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_0_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_0): ConvBn2d(\n",
       "    144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_1_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_0): ConvBn2d(\n",
       "    240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "  (net_encoder_layer3_0_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_0_1): Identity()\n",
       "  (net_encoder_layer3_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_1): Identity()\n",
       "  (net_encoder_layer3_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_0_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_0): ConvBn2d(\n",
       "    240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_0_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_1): Identity()\n",
       "  (net_encoder_layer3_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_1_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "  (net_encoder_layer3_2_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_0_1): Identity()\n",
       "  (net_encoder_layer3_2_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_1): Identity()\n",
       "  (net_encoder_layer3_2_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_2_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_1): Identity()\n",
       "  (net_encoder_layer3_2_stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_0): ConvBnReLU2d(\n",
       "    24, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_0): ConvBnReLU2d(\n",
       "    40, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_0): ConvBnReLU2d(\n",
       "    80, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_0): ConvBnReLU2d(\n",
       "    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_0): ConvBnReLU2d(\n",
       "    128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_0): ConvBnReLU2d(\n",
       "    64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_2): Identity()\n",
       "  (net_final_conv): Conv2d(\n",
       "    384, 1, kernel_size=(3, 3), stride=(1, 1), padding=same\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-127, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (fake_dequant_0): DeQuantStub()\n",
       "  (float_functional_simple_0): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_1): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_2): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_3): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_4): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_5): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_6): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_7): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_8): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_9): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_10): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_11): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_12): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_13): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_14): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_15): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptq_model_with_quantizer_tensor.to(\"cuda\")\n",
    "ptq_model_with_quantizer_channel.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post quantization calibration\n",
    "ptq_model_with_quantizer_tensor.apply(torch.quantization.disable_fake_quant)\n",
    "ptq_model_with_quantizer_tensor.apply(torch.quantization.enable_observer)\n",
    "ptq_model_with_quantizer_tensor = calibration(ptq_model_with_quantizer_tensor, 50, val_dataloader)\n",
    "\n",
    "ptq_model_with_quantizer_channel.apply(torch.quantization.disable_fake_quant)\n",
    "ptq_model_with_quantizer_channel.apply(torch.quantization.enable_observer)\n",
    "ptq_model_with_quantizer_channel = calibration(ptq_model_with_quantizer_tensor, 50, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QUNETLitModule(\n",
       "  (fake_quant_0): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_0): ConvBn2d(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0449], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.058018207550049, max_val=5.7242431640625)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0587], device='cuda:0'), zero_point=tensor([123], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.242151737213135, max_val=7.733004570007324)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_1): Identity()\n",
       "  (net_encoder_layer1_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0314], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=7.729601860046387)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_0): ConvBn2d(\n",
       "    32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0320], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.426515817642212, max_val=4.0841264724731445)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0956], device='cuda:0'), zero_point=tensor([142], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.548064231872559, max_val=10.818833351135254)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_1_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0435], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=10.81856632232666)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_1_0_block_1_fc1): Conv2d(\n",
       "    32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0113], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4103727340698242, max_val=1.4413390159606934)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0151], device='cuda:0'), zero_point=tensor([97], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4674309492111206, max_val=2.372910261154175)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0096], device='cuda:0'), zero_point=tensor([29], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2744198143482208, max_val=2.170625686645508)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_fc2): Conv2d(\n",
       "    8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0054], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5916793942451477, max_val=0.691929817199707)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0094], device='cuda:0'), zero_point=tensor([104], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9786236882209778, max_val=1.427304744720459)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_0): ConvBn2d(\n",
       "    32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0147], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.876504898071289, max_val=1.7068164348602295)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0774], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.918642044067383, max_val=9.817121505737305)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_0): ConvBn2d(\n",
       "    16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0072], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8208971619606018, max_val=0.9146025776863098)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0920], device='cuda:0'), zero_point=tensor([121], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.105051040649414, max_val=12.344745635986328)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0495], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=12.34468936920166)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_0): ConvBn2d(\n",
       "    96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0243], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.1035633087158203, max_val=2.856623649597168)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1130], device='cuda:0'), zero_point=tensor([119], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.441068649291992, max_val=15.375388145446777)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0614], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=15.37537670135498)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_0_block_2_fc1): Conv2d(\n",
       "    96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0175], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.23641300201416, max_val=1.8364068269729614)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0215], device='cuda:0'), zero_point=tensor([93], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.0023422241210938, max_val=3.4777631759643555)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0143], device='cuda:0'), zero_point=tensor([19], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27840012311935425, max_val=3.3672561645507812)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_fc2): Conv2d(\n",
       "    4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4841707944869995, max_val=0.39415696263313293)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0144], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7317074537277222, max_val=1.94966459274292)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_0): ConvBn2d(\n",
       "    96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0922805070877075, max_val=0.9243282675743103)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0788], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.300398826599121, max_val=10.782453536987305)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0077], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7756292223930359, max_val=0.9757888317108154)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0954], device='cuda:0'), zero_point=tensor([133], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.719738006591797, max_val=11.619293212890625)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0467], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=11.619180679321289)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0264], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.362685203552246, max_val=3.132139205932617)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1216], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.673567771911621, max_val=15.324178695678711)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0612], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=15.324172973632812)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_1_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0188], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.403042793273926, max_val=1.7017061710357666)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0375], device='cuda:0'), zero_point=tensor([159], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.973852634429932, max_val=3.5802011489868164)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0147], device='cuda:0'), zero_point=tensor([19], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2783867418766022, max_val=3.4766340255737305)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4701319634914398, max_val=0.32709673047065735)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0127], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6963039636611938, max_val=1.5350316762924194)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_0): ConvBn2d(\n",
       "    144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8381340503692627, max_val=0.8439326882362366)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0967], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.617661476135254, max_val=12.04259967803955)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "  (net_encoder_layer2_0_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0057], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7268950343132019, max_val=0.5689021348953247)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0993], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.613561630249023, max_val=12.70766830444336)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0509], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=12.707623481750488)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0131], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3869965076446533, max_val=1.6732805967330933)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0931], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.843472480773926, max_val=11.90871810913086)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0478], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=11.908628463745117)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_0_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0154], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9573631286621094, max_val=1.879593014717102)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0346], device='cuda:0'), zero_point=tensor([104], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.5848019123077393, max_val=5.240869998931885)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0215], device='cuda:0'), zero_point=tensor([13], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27844005823135376, max_val=5.211578845977783)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4879644215106964, max_val=0.3615483343601227)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0186], device='cuda:0'), zero_point=tensor([163], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0361430644989014, max_val=1.7070741653442383)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_0): ConvBn2d(\n",
       "    144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8346803784370422, max_val=0.8228920698165894)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0836], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.06290340423584, max_val=10.25397777557373)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7510979771614075, max_val=0.6438535451889038)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0863], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.893156051635742, max_val=11.114324569702148)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0447], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=11.114126205444336)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0113], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4170969724655151, max_val=1.4412744045257568)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0996], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.70135498046875, max_val=13.68852710723877)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0548], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=13.688508987426758)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_1_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0139], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4055018424987793, max_val=1.7671310901641846)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0440], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.653722286224365, max_val=5.5770583152771)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0229], device='cuda:0'), zero_point=tensor([12], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2784265875816345, max_val=5.55161190032959)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0034], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3406764566898346, max_val=0.4319320321083069)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0176], device='cuda:0'), zero_point=tensor([124], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.182663917541504, max_val=2.293999433517456)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_0): ConvBn2d(\n",
       "    240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9024238586425781, max_val=0.6945364475250244)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0988], device='cuda:0'), zero_point=tensor([131], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.923442840576172, max_val=12.275821685791016)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "  (net_encoder_layer3_0_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0036], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4651893973350525, max_val=0.45746976137161255)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0961], device='cuda:0'), zero_point=tensor([123], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.844799995422363, max_val=12.664347648620605)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_0_1): Identity()\n",
       "  (net_encoder_layer3_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0508], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=12.664299964904785)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0224], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.572762966156006, max_val=2.8588502407073975)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0959], device='cuda:0'), zero_point=tensor([123], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.821410179138184, max_val=12.630626678466797)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_1): Identity()\n",
       "  (net_encoder_layer3_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0506], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=12.63056755065918)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_0_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0121], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.543447494506836, max_val=1.4003231525421143)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0405], device='cuda:0'), zero_point=tensor([84], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.392587423324585, max_val=6.929008483886719)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0282], device='cuda:0'), zero_point=tensor([10], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27840012311935425, max_val=6.9207587242126465)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42404410243034363, max_val=0.3145257830619812)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0248], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.328929901123047, max_val=2.9832520484924316)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_0): ConvBn2d(\n",
       "    240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0072], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9126689434051514, max_val=0.9232595562934875)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0837], device='cuda:0'), zero_point=tensor([136], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.343422889709473, max_val=9.993321418762207)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6390053629875183, max_val=0.5376209616661072)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0841], device='cuda:0'), zero_point=tensor([122], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.298384666442871, max_val=11.143261909484863)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_0_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0448], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=11.143057823181152)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0266], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.386594295501709, max_val=2.431769847869873)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1101], device='cuda:0'), zero_point=tensor([117], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.873950958251953, max_val=15.205674171447754)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_1): Identity()\n",
       "  (net_encoder_layer3_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0607], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=15.205669403076172)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_1_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0107], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.368058204650879, max_val=1.2073206901550293)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0577], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.296091079711914, max_val=7.408161640167236)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0301], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2784581184387207, max_val=7.401487827301025)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4135446846485138, max_val=0.37192434072494507)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0394], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.732596397399902, max_val=5.314105987548828)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8329690098762512, max_val=0.7829931974411011)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0970], device='cuda:0'), zero_point=tensor([117], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.376526832580566, max_val=13.365242958068848)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "  (net_encoder_layer3_2_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3547547161579132, max_val=0.4134018123149872)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0968], device='cuda:0'), zero_point=tensor([124], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.02596664428711, max_val=12.65994644165039)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_0_1): Identity()\n",
       "  (net_encoder_layer3_2_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0507], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=12.659892082214355)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0363], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.627753734588623, max_val=3.1462533473968506)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1131], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.500494003295898, max_val=14.350303649902344)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_1): Identity()\n",
       "  (net_encoder_layer3_2_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0574], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27846458554267883, max_val=14.35029125213623)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_2_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0104], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3258647918701172, max_val=1.2849180698394775)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0581], device='cuda:0'), zero_point=tensor([97], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.632070064544678, max_val=9.195502281188965)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0371], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2784021198749542, max_val=9.19363021850586)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4529843032360077, max_val=0.48793476819992065)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0393], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.648168087005615, max_val=5.374480247497559)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0067], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8161272406578064, max_val=0.8564617037773132)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1028], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.996865272521973, max_val=13.206026077270508)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_1): Identity()\n",
       "  (net_encoder_layer3_2_stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_0): ConvBnReLU2d(\n",
       "    24, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0023], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.29836544394493103, max_val=0.2601902484893799)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0566], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.437165260314941)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_0): ConvBnReLU2d(\n",
       "    40, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0021], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27155598998069763, max_val=0.27232277393341064)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0533], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=13.603838920593262)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_0): ConvBnReLU2d(\n",
       "    80, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0015], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17596465349197388, max_val=0.18833346664905548)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0552], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.087987899780273)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0007], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.0767790824174881, max_val=0.09140922129154205)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0654], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.683719635009766)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0018], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.20948061347007751, max_val=0.23301711678504944)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0606], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.443732261657715)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_0): ConvBnReLU2d(\n",
       "    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0004], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03610607609152794, max_val=0.050745051354169846)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0654], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.683719635009766)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_0): ConvBnReLU2d(\n",
       "    128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0016], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1912088841199875, max_val=0.2030787169933319)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0657], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.743911743164062)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_0): ConvBnReLU2d(\n",
       "    64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0005], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07004806399345398, max_val=0.06318336725234985)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0654], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.683719635009766)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_2): Identity()\n",
       "  (net_final_conv): Conv2d(\n",
       "    384, 1, kernel_size=(3, 3), stride=(1, 1), padding=same\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0002], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.020986726507544518, max_val=0.027254510670900345)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.010330181568861008, max_val=1.0136843919754028)\n",
       "    )\n",
       "  )\n",
       "  (fake_dequant_0): DeQuantStub()\n",
       "  (float_functional_simple_0): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0260], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.223696768283844, max_val=6.4072160720825195)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_1): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0333], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24187925457954407, max_val=8.246535301208496)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_2): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0394], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2280496209859848, max_val=9.8164644241333)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_3): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1424], device='cuda:0'), zero_point=tensor([103], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.612997055053711, max_val=21.694671630859375)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_4): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0275], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23163901269435883, max_val=6.7798686027526855)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_5): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0390], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2522837817668915, max_val=9.696715354919434)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_6): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1646], device='cuda:0'), zero_point=tensor([131], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-21.505107879638672, max_val=20.466976165771484)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_7): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0302], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2631840407848358, max_val=7.4329609870910645)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_8): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0379], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2760995924472809, max_val=9.387971878051758)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_9): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1582], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-20.194297790527344, max_val=20.15613555908203)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_10): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0451], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.276842325925827, max_val=11.229742050170898)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_11): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2325], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-30.06793785095215, max_val=29.222291946411133)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_12): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0971], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=24.771345138549805)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_13): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0939], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=23.93973159790039)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_14): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0654], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.683719635009766)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_15): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0654], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.683719635009766)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disable observer and enable fake quantization to validate model with quantization error\n",
    "ptq_model_with_quantizer_tensor.apply(torch.quantization.disable_observer)\n",
    "ptq_model_with_quantizer_tensor.apply(torch.quantization.enable_fake_quant)\n",
    "# ptq_model_with_quantizer_tensor(next(iter(val_dataloader))[0].to(\"cuda\"))\n",
    "\n",
    "ptq_model_with_quantizer_channel.apply(torch.quantization.disable_observer)\n",
    "ptq_model_with_quantizer_channel.apply(torch.quantization.enable_fake_quant)\n",
    "# ptq_model_with_quantizer_channel(next(iter(val_dataloader))[0].to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **QAT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizer_per_tensor = QATQuantizer(\n",
    "    model,\n",
    "    torch.randn(1,3,52,52),\n",
    "    work_dir='quant_output',\n",
    "    config={\n",
    "        'asymmetric': True,\n",
    "        'backend': 'qnnpack',\n",
    "        \"disable_requantization_for_cat\": True,\n",
    "        'per_tensor': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model = quantizer_per_tensor.quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model = calibration(qat_model, 50, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QUNETLitModule(\n",
       "  (fake_quant_0): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_0): ConvBn2d(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0450], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.043659210205078, max_val=5.734618663787842)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0588], device='cuda:0'), zero_point=tensor([123], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.263873100280762, max_val=7.7389302253723145)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_1): Identity()\n",
       "  (net_encoder_layer1_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0313], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27977967262268066, max_val=7.6996355056762695)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_0): ConvBn2d(\n",
       "    32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0318], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4176135063171387, max_val=4.053454875946045)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0949], device='cuda:0'), zero_point=tensor([143], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.608550071716309, max_val=10.57917308807373)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_1_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0413], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2780262231826782, max_val=10.248092651367188)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_1_0_block_1_fc1): Conv2d(\n",
       "    32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0113], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4103727340698242, max_val=1.4413390159606934)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0139], device='cuda:0'), zero_point=tensor([94], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3046958446502686, max_val=2.2346441745758057)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0082], device='cuda:0'), zero_point=tensor([33], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2728944420814514, max_val=1.8259282112121582)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_fc2): Conv2d(\n",
       "    8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0054], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5916793942451477, max_val=0.691929817199707)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([104], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8420296311378479, max_val=1.215050458908081)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_0): ConvBn2d(\n",
       "    32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0147], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8761074542999268, max_val=1.7037972211837769)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0780], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.925348281860352, max_val=9.971457481384277)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_0): ConvBn2d(\n",
       "    16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0072], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8205308318138123, max_val=0.9189726710319519)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0909], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.867347717285156, max_val=12.309646606445312)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0490], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2795559763908386, max_val=12.22083568572998)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_0): ConvBn2d(\n",
       "    96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0242], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0850210189819336, max_val=2.81962251663208)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1150], device='cuda:0'), zero_point=tensor([114], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.060914993286133, max_val=16.254549026489258)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0647], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2792138457298279, max_val=16.216373443603516)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_0_block_2_fc1): Conv2d(\n",
       "    96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0175], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.23641300201416, max_val=1.8364068269729614)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0203], device='cuda:0'), zero_point=tensor([97], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9634848833084106, max_val=3.217726469039917)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([24], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.279704749584198, max_val=2.7380259037017822)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_fc2): Conv2d(\n",
       "    4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4841707944869995, max_val=0.39415696263313293)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0121], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4247878789901733, max_val=1.656044602394104)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_0): ConvBn2d(\n",
       "    96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0938681364059448, max_val=0.9286837577819824)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0787], device='cuda:0'), zero_point=tensor([115], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.086021423339844, max_val=10.98910903930664)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0077], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7781527638435364, max_val=0.9777565002441406)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0951], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.778800964355469, max_val=11.470104217529297)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0456], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2789490818977356, max_val=11.345874786376953)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0262], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.3364148139953613, max_val=3.092686414718628)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1189], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.6409273147583, max_val=14.688051223754883)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0573], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2780535817146301, max_val=14.322757720947266)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_1_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0188], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.403042793273926, max_val=1.7017061710357666)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0371], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.9739251136779785, max_val=3.491455316543579)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0128], device='cuda:0'), zero_point=tensor([22], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2789660394191742, max_val=2.985771417617798)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4701319634914398, max_val=0.32709673047065735)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0114], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5280524492263794, max_val=1.3823466300964355)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_0): ConvBn2d(\n",
       "    144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8395814299583435, max_val=0.8451184034347534)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0942], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.143216133117676, max_val=11.88066577911377)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "  (net_encoder_layer2_0_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0057], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7239266633987427, max_val=0.5671959519386292)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0994], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.683080673217773, max_val=12.665587425231934)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0504], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.280159592628479, max_val=12.576397895812988)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0130], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3896872997283936, max_val=1.6630841493606567)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0943], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.088597297668457, max_val=11.948124885559082)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0476], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2798846662044525, max_val=11.861300468444824)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_0_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0154], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9573631286621094, max_val=1.879593014717102)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0349], device='cuda:0'), zero_point=tensor([107], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.7341556549072266, max_val=5.158235549926758)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0208], device='cuda:0'), zero_point=tensor([13], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27936652302742004, max_val=5.031614303588867)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4879644215106964, max_val=0.3615483343601227)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0175], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.805467128753662, max_val=1.6490662097930908)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_0): ConvBn2d(\n",
       "    144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.835041880607605, max_val=0.8234864473342896)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0848], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.352572441101074, max_val=10.260930061340332)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.750647246837616, max_val=0.6420109868049622)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0873], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.293466567993164, max_val=10.977378845214844)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0439], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2800154387950897, max_val=10.919029235839844)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4182199239730835, max_val=1.4288922548294067)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1004], device='cuda:0'), zero_point=tensor([117], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.74235725402832, max_val=13.856621742248535)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0549], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2791571617126465, max_val=13.730056762695312)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_1_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0139], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4055018424987793, max_val=1.7671310901641846)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0447], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.906805515289307, max_val=5.486788749694824)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0222], device='cuda:0'), zero_point=tensor([13], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28020039200782776, max_val=5.379629135131836)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0034], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3406764566898346, max_val=0.4319320321083069)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0172], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2264726161956787, max_val=2.1635994911193848)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_0): ConvBn2d(\n",
       "    240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9003563523292542, max_val=0.6918697953224182)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1000], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.192778587341309, max_val=12.316064834594727)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "  (net_encoder_layer3_0_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4676007926464081, max_val=0.4575655460357666)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1002], device='cuda:0'), zero_point=tensor([124], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.412806510925293, max_val=13.140999794006348)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_0_1): Identity()\n",
       "  (net_encoder_layer3_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0524], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2790752947330475, max_val=13.083654403686523)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0224], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5601437091827393, max_val=2.8545138835906982)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0968], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.57463550567627, max_val=13.11510181427002)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_1): Identity()\n",
       "  (net_encoder_layer3_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0521], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2784714102745056, max_val=13.002901077270508)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_0_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0121], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.543447494506836, max_val=1.4003231525421143)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0379], device='cuda:0'), zero_point=tensor([92], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4827473163604736, max_val=6.180971622467041)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0234], device='cuda:0'), zero_point=tensor([12], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2797144949436188, max_val=5.6879730224609375)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42404410243034363, max_val=0.3145257830619812)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0219], device='cuda:0'), zero_point=tensor([133], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9138174057006836, max_val=2.6629021167755127)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_0): ConvBn2d(\n",
       "    240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0073], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9153355956077576, max_val=0.9257211089134216)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0873], device='cuda:0'), zero_point=tensor([136], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.863374710083008, max_val=10.400520324707031)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6364786624908447, max_val=0.5366501212120056)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0872], device='cuda:0'), zero_point=tensor([121], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.527286529541016, max_val=11.70177173614502)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_0_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0468], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2798554599285126, max_val=11.642064094543457)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0265], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.381244421005249, max_val=2.4333271980285645)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1124], device='cuda:0'), zero_point=tensor([121], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.570856094360352, max_val=15.088375091552734)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_1): Identity()\n",
       "  (net_encoder_layer3_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0597], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27982524037361145, max_val=14.936224937438965)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_1_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0107], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.368058204650879, max_val=1.2073206901550293)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0551], device='cuda:0'), zero_point=tensor([119], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.525923728942871, max_val=7.513123989105225)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0303], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2791365683078766, max_val=7.436128616333008)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4135446846485138, max_val=0.37192434072494507)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0428], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.061733245849609, max_val=5.859547138214111)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8316009044647217, max_val=0.7828212380409241)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1016], device='cuda:0'), zero_point=tensor([115], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.732812881469727, max_val=14.175081253051758)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "  (net_encoder_layer3_2_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35769110918045044, max_val=0.4149148762226105)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1006], device='cuda:0'), zero_point=tensor([124], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.505239486694336, max_val=13.142346382141113)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_0_1): Identity()\n",
       "  (net_encoder_layer3_2_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0522], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2796258330345154, max_val=13.03331184387207)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0359], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.582597732543945, max_val=3.159688949584961)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1161], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.976546287536621, max_val=15.619869232177734)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_1): Identity()\n",
       "  (net_encoder_layer3_2_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0620], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2794046998023987, max_val=15.535189628601074)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_2_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0104], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3258647918701172, max_val=1.2849180698394775)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0496], device='cuda:0'), zero_point=tensor([109], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.40542459487915, max_val=7.235474109649658)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0274], device='cuda:0'), zero_point=tensor([10], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2799387574195862, max_val=6.7173662185668945)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4529843032360077, max_val=0.48793476819992065)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0375], device='cuda:0'), zero_point=tensor([116], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.369065284729004, max_val=5.205258369445801)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0067], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.810765266418457, max_val=0.8555827140808105)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1061], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.539660453796387, max_val=13.508418083190918)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_1): Identity()\n",
       "  (net_encoder_layer3_2_stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_0): ConvBnReLU2d(\n",
       "    24, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0023], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.297598272562027, max_val=0.26047536730766296)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0567], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.449585914611816)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_0): ConvBnReLU2d(\n",
       "    40, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0021], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2729571759700775, max_val=0.2730327248573303)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0562], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.338400840759277)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_0): ConvBnReLU2d(\n",
       "    80, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0015], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17631450295448303, max_val=0.18941503763198853)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0582], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.832235336303711)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0007], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07744815945625305, max_val=0.09268926084041595)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0018], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2091578096151352, max_val=0.23418985307216644)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0636], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.206953048706055)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_0): ConvBnReLU2d(\n",
       "    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0004], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03609912097454071, max_val=0.05075689032673836)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_0): ConvBnReLU2d(\n",
       "    128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0016], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1917170137166977, max_val=0.2031281590461731)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.617536544799805)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_0): ConvBnReLU2d(\n",
       "    64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0005], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07010912150144577, max_val=0.06318911910057068)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_2): Identity()\n",
       "  (net_final_conv): Conv2d(\n",
       "    384, 1, kernel_size=(3, 3), stride=(1, 1), padding=same\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0002], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.020986726507544518, max_val=0.027254510670900345)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.010998968034982681, max_val=1.0446158647537231)\n",
       "    )\n",
       "  )\n",
       "  (fake_dequant_0): DeQuantStub()\n",
       "  (float_functional_simple_0): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0253], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21868912875652313, max_val=6.226154327392578)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_1): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0325], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2288961112499237, max_val=8.05055046081543)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_2): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0369], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.223818838596344, max_val=9.18370246887207)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_3): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1421], device='cuda:0'), zero_point=tensor([101], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.349632263183594, max_val=21.89630699157715)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_4): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0275], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23703870177268982, max_val=6.766177177429199)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_5): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0395], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24966809153556824, max_val=9.833389282226562)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_6): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1682], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-22.456207275390625, max_val=20.423616409301758)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_7): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0314], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24751269817352295, max_val=7.767127513885498)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_8): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0404], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30226606130599976, max_val=9.988883018493652)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_9): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1707], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-21.61538314819336, max_val=21.90613555908203)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_10): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0451], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25835850834846497, max_val=11.240936279296875)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_11): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2577], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-32.49716567993164, max_val=33.210166931152344)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_12): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=26.71375274658203)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_13): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1005], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=25.63890266418457)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_14): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_15): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qat_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QUNETLitModule(\n",
       "  (fake_quant_0): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_0): ConvBn2d(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0450], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.043659210205078, max_val=5.734618663787842)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0588], device='cuda:0'), zero_point=tensor([123], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.263873100280762, max_val=7.7389302253723145)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_1): Identity()\n",
       "  (net_encoder_layer1_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0313], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27977967262268066, max_val=7.6996355056762695)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_0): ConvBn2d(\n",
       "    32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0318], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4176135063171387, max_val=4.053454875946045)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0949], device='cuda:0'), zero_point=tensor([143], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.608550071716309, max_val=10.57917308807373)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_1_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0413], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2780262231826782, max_val=10.248092651367188)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_1_0_block_1_fc1): Conv2d(\n",
       "    32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0113], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4103727340698242, max_val=1.4413390159606934)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0139], device='cuda:0'), zero_point=tensor([94], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3046958446502686, max_val=2.2346441745758057)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0082], device='cuda:0'), zero_point=tensor([33], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2728944420814514, max_val=1.8259282112121582)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_fc2): Conv2d(\n",
       "    8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0054], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5916793942451477, max_val=0.691929817199707)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([104], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8420296311378479, max_val=1.215050458908081)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_0): ConvBn2d(\n",
       "    32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0147], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8761074542999268, max_val=1.7037972211837769)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0780], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.925348281860352, max_val=9.971457481384277)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_0): ConvBn2d(\n",
       "    16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0072], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8205308318138123, max_val=0.9189726710319519)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0909], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.867347717285156, max_val=12.309646606445312)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0490], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2795559763908386, max_val=12.22083568572998)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_0): ConvBn2d(\n",
       "    96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0242], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0850210189819336, max_val=2.81962251663208)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1150], device='cuda:0'), zero_point=tensor([114], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.060914993286133, max_val=16.254549026489258)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0647], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2792138457298279, max_val=16.216373443603516)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_0_block_2_fc1): Conv2d(\n",
       "    96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0175], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.23641300201416, max_val=1.8364068269729614)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0203], device='cuda:0'), zero_point=tensor([97], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9634848833084106, max_val=3.217726469039917)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([24], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.279704749584198, max_val=2.7380259037017822)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_fc2): Conv2d(\n",
       "    4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4841707944869995, max_val=0.39415696263313293)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0121], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4247878789901733, max_val=1.656044602394104)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_0): ConvBn2d(\n",
       "    96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0938681364059448, max_val=0.9286837577819824)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0787], device='cuda:0'), zero_point=tensor([115], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.086021423339844, max_val=10.98910903930664)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0077], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7781527638435364, max_val=0.9777565002441406)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0951], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.778800964355469, max_val=11.470104217529297)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0456], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2789490818977356, max_val=11.345874786376953)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0262], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.3364148139953613, max_val=3.092686414718628)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1189], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.6409273147583, max_val=14.688051223754883)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0573], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2780535817146301, max_val=14.322757720947266)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_1_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0188], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.403042793273926, max_val=1.7017061710357666)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0371], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.9739251136779785, max_val=3.491455316543579)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0128], device='cuda:0'), zero_point=tensor([22], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2789660394191742, max_val=2.985771417617798)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4701319634914398, max_val=0.32709673047065735)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0114], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5280524492263794, max_val=1.3823466300964355)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_0): ConvBn2d(\n",
       "    144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8395814299583435, max_val=0.8451184034347534)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0942], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.143216133117676, max_val=11.88066577911377)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "  (net_encoder_layer2_0_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0057], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7239266633987427, max_val=0.5671959519386292)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0994], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.683080673217773, max_val=12.665587425231934)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0504], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.280159592628479, max_val=12.576397895812988)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0130], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3896872997283936, max_val=1.6630841493606567)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0943], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.088597297668457, max_val=11.948124885559082)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0476], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2798846662044525, max_val=11.861300468444824)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_0_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0154], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9573631286621094, max_val=1.879593014717102)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0349], device='cuda:0'), zero_point=tensor([107], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.7341556549072266, max_val=5.158235549926758)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0208], device='cuda:0'), zero_point=tensor([13], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27936652302742004, max_val=5.031614303588867)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4879644215106964, max_val=0.3615483343601227)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0175], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.805467128753662, max_val=1.6490662097930908)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_0): ConvBn2d(\n",
       "    144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.835041880607605, max_val=0.8234864473342896)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0848], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.352572441101074, max_val=10.260930061340332)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.750647246837616, max_val=0.6420109868049622)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0873], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.293466567993164, max_val=10.977378845214844)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0439], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2800154387950897, max_val=10.919029235839844)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4182199239730835, max_val=1.4288922548294067)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1004], device='cuda:0'), zero_point=tensor([117], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.74235725402832, max_val=13.856621742248535)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0549], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2791571617126465, max_val=13.730056762695312)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_1_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0139], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4055018424987793, max_val=1.7671310901641846)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0447], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.906805515289307, max_val=5.486788749694824)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0222], device='cuda:0'), zero_point=tensor([13], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28020039200782776, max_val=5.379629135131836)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0034], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3406764566898346, max_val=0.4319320321083069)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0172], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2264726161956787, max_val=2.1635994911193848)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_0): ConvBn2d(\n",
       "    240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9003563523292542, max_val=0.6918697953224182)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1000], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.192778587341309, max_val=12.316064834594727)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "  (net_encoder_layer3_0_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4676007926464081, max_val=0.4575655460357666)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1002], device='cuda:0'), zero_point=tensor([124], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.412806510925293, max_val=13.140999794006348)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_0_1): Identity()\n",
       "  (net_encoder_layer3_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0524], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2790752947330475, max_val=13.083654403686523)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0224], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5601437091827393, max_val=2.8545138835906982)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0968], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.57463550567627, max_val=13.11510181427002)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_1): Identity()\n",
       "  (net_encoder_layer3_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0521], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2784714102745056, max_val=13.002901077270508)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_0_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0121], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.543447494506836, max_val=1.4003231525421143)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0379], device='cuda:0'), zero_point=tensor([92], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4827473163604736, max_val=6.180971622467041)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0234], device='cuda:0'), zero_point=tensor([12], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2797144949436188, max_val=5.6879730224609375)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42404410243034363, max_val=0.3145257830619812)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0219], device='cuda:0'), zero_point=tensor([133], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9138174057006836, max_val=2.6629021167755127)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_0): ConvBn2d(\n",
       "    240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0073], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9153355956077576, max_val=0.9257211089134216)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0873], device='cuda:0'), zero_point=tensor([136], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.863374710083008, max_val=10.400520324707031)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6364786624908447, max_val=0.5366501212120056)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0872], device='cuda:0'), zero_point=tensor([121], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.527286529541016, max_val=11.70177173614502)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_0_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0468], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2798554599285126, max_val=11.642064094543457)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0265], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.381244421005249, max_val=2.4333271980285645)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1124], device='cuda:0'), zero_point=tensor([121], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.570856094360352, max_val=15.088375091552734)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_1): Identity()\n",
       "  (net_encoder_layer3_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0597], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27982524037361145, max_val=14.936224937438965)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_1_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0107], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.368058204650879, max_val=1.2073206901550293)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0551], device='cuda:0'), zero_point=tensor([119], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.525923728942871, max_val=7.513123989105225)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0303], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2791365683078766, max_val=7.436128616333008)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4135446846485138, max_val=0.37192434072494507)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0428], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.061733245849609, max_val=5.859547138214111)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8316009044647217, max_val=0.7828212380409241)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1016], device='cuda:0'), zero_point=tensor([115], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.732812881469727, max_val=14.175081253051758)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "  (net_encoder_layer3_2_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35769110918045044, max_val=0.4149148762226105)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1006], device='cuda:0'), zero_point=tensor([124], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.505239486694336, max_val=13.142346382141113)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_0_1): Identity()\n",
       "  (net_encoder_layer3_2_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0522], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2796258330345154, max_val=13.03331184387207)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0359], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.582597732543945, max_val=3.159688949584961)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1161], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.976546287536621, max_val=15.619869232177734)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_1): Identity()\n",
       "  (net_encoder_layer3_2_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0620], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2794046998023987, max_val=15.535189628601074)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_2_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0104], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3258647918701172, max_val=1.2849180698394775)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0496], device='cuda:0'), zero_point=tensor([109], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.40542459487915, max_val=7.235474109649658)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0274], device='cuda:0'), zero_point=tensor([10], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2799387574195862, max_val=6.7173662185668945)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4529843032360077, max_val=0.48793476819992065)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0375], device='cuda:0'), zero_point=tensor([116], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.369065284729004, max_val=5.205258369445801)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0067], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.810765266418457, max_val=0.8555827140808105)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1061], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.539660453796387, max_val=13.508418083190918)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_1): Identity()\n",
       "  (net_encoder_layer3_2_stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_0): ConvBnReLU2d(\n",
       "    24, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0023], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.297598272562027, max_val=0.26047536730766296)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0567], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.449585914611816)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_0): ConvBnReLU2d(\n",
       "    40, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0021], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2729571759700775, max_val=0.2730327248573303)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0562], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.338400840759277)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_0): ConvBnReLU2d(\n",
       "    80, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0015], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17631450295448303, max_val=0.18941503763198853)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0582], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.832235336303711)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0007], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07744815945625305, max_val=0.09268926084041595)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0018], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2091578096151352, max_val=0.23418985307216644)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0636], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.206953048706055)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_0): ConvBnReLU2d(\n",
       "    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0004], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03609912097454071, max_val=0.05075689032673836)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_0): ConvBnReLU2d(\n",
       "    128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0016], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1917170137166977, max_val=0.2031281590461731)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.617536544799805)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_0): ConvBnReLU2d(\n",
       "    64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0005], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07010912150144577, max_val=0.06318911910057068)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_2): Identity()\n",
       "  (net_final_conv): Conv2d(\n",
       "    384, 1, kernel_size=(3, 3), stride=(1, 1), padding=same\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0002], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.020986726507544518, max_val=0.027254510670900345)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.010998968034982681, max_val=1.0446158647537231)\n",
       "    )\n",
       "  )\n",
       "  (fake_dequant_0): DeQuantStub()\n",
       "  (float_functional_simple_0): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0253], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21868912875652313, max_val=6.226154327392578)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_1): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0325], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2288961112499237, max_val=8.05055046081543)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_2): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0369], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.223818838596344, max_val=9.18370246887207)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_3): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1421], device='cuda:0'), zero_point=tensor([101], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.349632263183594, max_val=21.89630699157715)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_4): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0275], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23703870177268982, max_val=6.766177177429199)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_5): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0395], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24966809153556824, max_val=9.833389282226562)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_6): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1682], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-22.456207275390625, max_val=20.423616409301758)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_7): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0314], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24751269817352295, max_val=7.767127513885498)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_8): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0404], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30226606130599976, max_val=9.988883018493652)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_9): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1707], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-21.61538314819336, max_val=21.90613555908203)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_10): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0451], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25835850834846497, max_val=11.240936279296875)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_11): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2577], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-32.49716567993164, max_val=33.210166931152344)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_12): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=26.71375274658203)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_13): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1005], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=25.63890266418457)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_14): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_15): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qat_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QUNETLitModule(\n",
       "  (fake_quant_0): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_0): ConvBn2d(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0450], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.043659210205078, max_val=5.734618663787842)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0588], device='cuda:0'), zero_point=tensor([123], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.263873100280762, max_val=7.7389302253723145)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_1): Identity()\n",
       "  (net_encoder_layer1_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0313], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27977967262268066, max_val=7.6996355056762695)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_0): ConvBn2d(\n",
       "    32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0318], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4176135063171387, max_val=4.053454875946045)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0949], device='cuda:0'), zero_point=tensor([143], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.608550071716309, max_val=10.57917308807373)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_1_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0413], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2780262231826782, max_val=10.248092651367188)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_1_0_block_1_fc1): Conv2d(\n",
       "    32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0113], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4103727340698242, max_val=1.4413390159606934)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0139], device='cuda:0'), zero_point=tensor([94], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3046958446502686, max_val=2.2346441745758057)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0082], device='cuda:0'), zero_point=tensor([33], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2728944420814514, max_val=1.8259282112121582)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_fc2): Conv2d(\n",
       "    8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0054], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5916793942451477, max_val=0.691929817199707)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([104], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8420296311378479, max_val=1.215050458908081)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_0): ConvBn2d(\n",
       "    32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0147], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8761074542999268, max_val=1.7037972211837769)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0780], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.925348281860352, max_val=9.971457481384277)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_0): ConvBn2d(\n",
       "    16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0072], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8205308318138123, max_val=0.9189726710319519)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0909], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.867347717285156, max_val=12.309646606445312)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0490], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2795559763908386, max_val=12.22083568572998)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_0): ConvBn2d(\n",
       "    96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0242], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0850210189819336, max_val=2.81962251663208)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1150], device='cuda:0'), zero_point=tensor([114], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.060914993286133, max_val=16.254549026489258)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0647], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2792138457298279, max_val=16.216373443603516)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_0_block_2_fc1): Conv2d(\n",
       "    96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0175], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.23641300201416, max_val=1.8364068269729614)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0203], device='cuda:0'), zero_point=tensor([97], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9634848833084106, max_val=3.217726469039917)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([24], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.279704749584198, max_val=2.7380259037017822)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_fc2): Conv2d(\n",
       "    4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4841707944869995, max_val=0.39415696263313293)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0121], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4247878789901733, max_val=1.656044602394104)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_0): ConvBn2d(\n",
       "    96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0938681364059448, max_val=0.9286837577819824)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0787], device='cuda:0'), zero_point=tensor([115], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.086021423339844, max_val=10.98910903930664)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0077], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7781527638435364, max_val=0.9777565002441406)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0951], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.778800964355469, max_val=11.470104217529297)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0456], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2789490818977356, max_val=11.345874786376953)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0262], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.3364148139953613, max_val=3.092686414718628)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1189], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.6409273147583, max_val=14.688051223754883)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0573], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2780535817146301, max_val=14.322757720947266)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_1_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0188], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.403042793273926, max_val=1.7017061710357666)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0371], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.9739251136779785, max_val=3.491455316543579)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0128], device='cuda:0'), zero_point=tensor([22], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2789660394191742, max_val=2.985771417617798)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4701319634914398, max_val=0.32709673047065735)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0114], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5280524492263794, max_val=1.3823466300964355)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_0): ConvBn2d(\n",
       "    144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8395814299583435, max_val=0.8451184034347534)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0942], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.143216133117676, max_val=11.88066577911377)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "  (net_encoder_layer2_0_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0057], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7239266633987427, max_val=0.5671959519386292)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0994], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.683080673217773, max_val=12.665587425231934)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0504], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.280159592628479, max_val=12.576397895812988)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0130], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3896872997283936, max_val=1.6630841493606567)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0943], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.088597297668457, max_val=11.948124885559082)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0476], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2798846662044525, max_val=11.861300468444824)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_0_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0154], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9573631286621094, max_val=1.879593014717102)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0349], device='cuda:0'), zero_point=tensor([107], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.7341556549072266, max_val=5.158235549926758)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0208], device='cuda:0'), zero_point=tensor([13], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27936652302742004, max_val=5.031614303588867)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4879644215106964, max_val=0.3615483343601227)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0175], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.805467128753662, max_val=1.6490662097930908)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_0): ConvBn2d(\n",
       "    144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.835041880607605, max_val=0.8234864473342896)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0848], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.352572441101074, max_val=10.260930061340332)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.750647246837616, max_val=0.6420109868049622)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0873], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.293466567993164, max_val=10.977378845214844)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0439], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2800154387950897, max_val=10.919029235839844)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4182199239730835, max_val=1.4288922548294067)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1004], device='cuda:0'), zero_point=tensor([117], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.74235725402832, max_val=13.856621742248535)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0549], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2791571617126465, max_val=13.730056762695312)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_1_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0139], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4055018424987793, max_val=1.7671310901641846)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0447], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.906805515289307, max_val=5.486788749694824)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0222], device='cuda:0'), zero_point=tensor([13], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28020039200782776, max_val=5.379629135131836)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0034], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3406764566898346, max_val=0.4319320321083069)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0172], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2264726161956787, max_val=2.1635994911193848)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_0): ConvBn2d(\n",
       "    240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9003563523292542, max_val=0.6918697953224182)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1000], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.192778587341309, max_val=12.316064834594727)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "  (net_encoder_layer3_0_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4676007926464081, max_val=0.4575655460357666)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1002], device='cuda:0'), zero_point=tensor([124], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.412806510925293, max_val=13.140999794006348)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_0_1): Identity()\n",
       "  (net_encoder_layer3_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0524], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2790752947330475, max_val=13.083654403686523)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0224], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5601437091827393, max_val=2.8545138835906982)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0968], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.57463550567627, max_val=13.11510181427002)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_1): Identity()\n",
       "  (net_encoder_layer3_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0521], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2784714102745056, max_val=13.002901077270508)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_0_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0121], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.543447494506836, max_val=1.4003231525421143)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0379], device='cuda:0'), zero_point=tensor([92], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4827473163604736, max_val=6.180971622467041)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0234], device='cuda:0'), zero_point=tensor([12], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2797144949436188, max_val=5.6879730224609375)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42404410243034363, max_val=0.3145257830619812)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0219], device='cuda:0'), zero_point=tensor([133], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9138174057006836, max_val=2.6629021167755127)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_0): ConvBn2d(\n",
       "    240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0073], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9153355956077576, max_val=0.9257211089134216)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0873], device='cuda:0'), zero_point=tensor([136], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.863374710083008, max_val=10.400520324707031)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6364786624908447, max_val=0.5366501212120056)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0872], device='cuda:0'), zero_point=tensor([121], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.527286529541016, max_val=11.70177173614502)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_0_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0468], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2798554599285126, max_val=11.642064094543457)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0265], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.381244421005249, max_val=2.4333271980285645)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1124], device='cuda:0'), zero_point=tensor([121], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.570856094360352, max_val=15.088375091552734)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_1): Identity()\n",
       "  (net_encoder_layer3_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0597], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27982524037361145, max_val=14.936224937438965)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_1_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0107], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.368058204650879, max_val=1.2073206901550293)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0551], device='cuda:0'), zero_point=tensor([119], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.525923728942871, max_val=7.513123989105225)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0303], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2791365683078766, max_val=7.436128616333008)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4135446846485138, max_val=0.37192434072494507)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0428], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.061733245849609, max_val=5.859547138214111)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8316009044647217, max_val=0.7828212380409241)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1016], device='cuda:0'), zero_point=tensor([115], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.732812881469727, max_val=14.175081253051758)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "  (net_encoder_layer3_2_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35769110918045044, max_val=0.4149148762226105)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1006], device='cuda:0'), zero_point=tensor([124], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.505239486694336, max_val=13.142346382141113)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_0_1): Identity()\n",
       "  (net_encoder_layer3_2_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0522], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2796258330345154, max_val=13.03331184387207)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0359], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.582597732543945, max_val=3.159688949584961)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1161], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.976546287536621, max_val=15.619869232177734)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_1): Identity()\n",
       "  (net_encoder_layer3_2_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0620], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2794046998023987, max_val=15.535189628601074)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_2_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0104], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3258647918701172, max_val=1.2849180698394775)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0496], device='cuda:0'), zero_point=tensor([109], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.40542459487915, max_val=7.235474109649658)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0274], device='cuda:0'), zero_point=tensor([10], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2799387574195862, max_val=6.7173662185668945)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4529843032360077, max_val=0.48793476819992065)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0375], device='cuda:0'), zero_point=tensor([116], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.369065284729004, max_val=5.205258369445801)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0067], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.810765266418457, max_val=0.8555827140808105)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1061], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.539660453796387, max_val=13.508418083190918)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_1): Identity()\n",
       "  (net_encoder_layer3_2_stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_0): ConvBnReLU2d(\n",
       "    24, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0023], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.297598272562027, max_val=0.26047536730766296)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0567], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.449585914611816)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_0): ConvBnReLU2d(\n",
       "    40, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0021], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2729571759700775, max_val=0.2730327248573303)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0562], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.338400840759277)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_0): ConvBnReLU2d(\n",
       "    80, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0015], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17631450295448303, max_val=0.18941503763198853)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0582], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.832235336303711)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0007], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07744815945625305, max_val=0.09268926084041595)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0018], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2091578096151352, max_val=0.23418985307216644)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0636], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.206953048706055)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_0): ConvBnReLU2d(\n",
       "    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0004], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03609912097454071, max_val=0.05075689032673836)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_0): ConvBnReLU2d(\n",
       "    128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0016], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1917170137166977, max_val=0.2031281590461731)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.617536544799805)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_0): ConvBnReLU2d(\n",
       "    64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0005], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07010912150144577, max_val=0.06318911910057068)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_2): Identity()\n",
       "  (net_final_conv): Conv2d(\n",
       "    384, 1, kernel_size=(3, 3), stride=(1, 1), padding=same\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0002], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.020986726507544518, max_val=0.027254510670900345)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.010998968034982681, max_val=1.0446158647537231)\n",
       "    )\n",
       "  )\n",
       "  (fake_dequant_0): DeQuantStub()\n",
       "  (float_functional_simple_0): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0253], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21868912875652313, max_val=6.226154327392578)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_1): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0325], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2288961112499237, max_val=8.05055046081543)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_2): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0369], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.223818838596344, max_val=9.18370246887207)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_3): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1421], device='cuda:0'), zero_point=tensor([101], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.349632263183594, max_val=21.89630699157715)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_4): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0275], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23703870177268982, max_val=6.766177177429199)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_5): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0395], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24966809153556824, max_val=9.833389282226562)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_6): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1682], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-22.456207275390625, max_val=20.423616409301758)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_7): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0314], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24751269817352295, max_val=7.767127513885498)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_8): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0404], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30226606130599976, max_val=9.988883018493652)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_9): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1707], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-21.61538314819336, max_val=21.90613555908203)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_10): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0451], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25835850834846497, max_val=11.240936279296875)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_11): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2577], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-32.49716567993164, max_val=33.210166931152344)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_12): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=26.71375274658203)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_13): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1005], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=25.63890266418457)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_14): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_15): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qat_model.apply(torch.quantization.enable_fake_quant)\n",
    "qat_model.apply(torch.quantization.enable_observer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QUNETLitModule(\n",
       "  (fake_quant_0): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_0): ConvBn2d(\n",
       "    3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0450], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.043659210205078, max_val=5.734618663787842)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0588], device='cuda:0'), zero_point=tensor([123], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.263873100280762, max_val=7.7389302253723145)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_0_1): Identity()\n",
       "  (net_encoder_layer1_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0313], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27977967262268066, max_val=7.6996355056762695)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_0): ConvBn2d(\n",
       "    32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0318], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4176135063171387, max_val=4.053454875946045)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0949], device='cuda:0'), zero_point=tensor([143], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.608550071716309, max_val=10.57917308807373)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_1_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0413], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2780262231826782, max_val=10.248092651367188)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_1_0_block_1_fc1): Conv2d(\n",
       "    32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0113], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4103727340698242, max_val=1.4413390159606934)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0139], device='cuda:0'), zero_point=tensor([94], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3046958446502686, max_val=2.2346441745758057)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0082], device='cuda:0'), zero_point=tensor([33], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2728944420814514, max_val=1.8259282112121582)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_fc2): Conv2d(\n",
       "    8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0054], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5916793942451477, max_val=0.691929817199707)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([104], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8420296311378479, max_val=1.215050458908081)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_1_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_0): ConvBn2d(\n",
       "    32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0147], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8761074542999268, max_val=1.7037972211837769)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0780], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.925348281860352, max_val=9.971457481384277)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_1_0_block_2_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_0): ConvBn2d(\n",
       "    16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0072], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8205308318138123, max_val=0.9189726710319519)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0909], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.867347717285156, max_val=12.309646606445312)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0490], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2795559763908386, max_val=12.22083568572998)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_0): ConvBn2d(\n",
       "    96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0242], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0850210189819336, max_val=2.81962251663208)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1150], device='cuda:0'), zero_point=tensor([114], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.060914993286133, max_val=16.254549026489258)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0647], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2792138457298279, max_val=16.216373443603516)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_0_block_2_fc1): Conv2d(\n",
       "    96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0175], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.23641300201416, max_val=1.8364068269729614)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0203], device='cuda:0'), zero_point=tensor([97], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9634848833084106, max_val=3.217726469039917)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([24], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.279704749584198, max_val=2.7380259037017822)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_fc2): Conv2d(\n",
       "    4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4841707944869995, max_val=0.39415696263313293)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0121], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4247878789901733, max_val=1.656044602394104)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_0): ConvBn2d(\n",
       "    96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0938681364059448, max_val=0.9286837577819824)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0787], device='cuda:0'), zero_point=tensor([115], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.086021423339844, max_val=10.98910903930664)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0077], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7781527638435364, max_val=0.9777565002441406)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0951], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.778800964355469, max_val=11.470104217529297)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0456], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2789490818977356, max_val=11.345874786376953)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0262], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.3364148139953613, max_val=3.092686414718628)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1189], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.6409273147583, max_val=14.688051223754883)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer1_2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0573], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2780535817146301, max_val=14.322757720947266)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer1_2_1_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0188], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.403042793273926, max_val=1.7017061710357666)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0371], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.9739251136779785, max_val=3.491455316543579)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0128], device='cuda:0'), zero_point=tensor([22], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2789660394191742, max_val=2.985771417617798)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4701319634914398, max_val=0.32709673047065735)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0114], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5280524492263794, max_val=1.3823466300964355)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_0): ConvBn2d(\n",
       "    144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0066], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8395814299583435, max_val=0.8451184034347534)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0942], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.143216133117676, max_val=11.88066577911377)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer1_2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer1_2_1_stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "  (net_encoder_layer2_0_block_0_0): ConvBn2d(\n",
       "    24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0057], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7239266633987427, max_val=0.5671959519386292)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0994], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.683080673217773, max_val=12.665587425231934)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_0_1): Identity()\n",
       "  (net_encoder_layer2_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0504], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.280159592628479, max_val=12.576397895812988)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_0): ConvBn2d(\n",
       "    144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False\n",
       "    (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0130], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3896872997283936, max_val=1.6630841493606567)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0943], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.088597297668457, max_val=11.948124885559082)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_1_1): Identity()\n",
       "  (net_encoder_layer2_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0476], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2798846662044525, max_val=11.861300468444824)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_0_block_2_fc1): Conv2d(\n",
       "    144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0154], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.9573631286621094, max_val=1.879593014717102)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0349], device='cuda:0'), zero_point=tensor([107], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.7341556549072266, max_val=5.158235549926758)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0208], device='cuda:0'), zero_point=tensor([13], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27936652302742004, max_val=5.031614303588867)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_fc2): Conv2d(\n",
       "    6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4879644215106964, max_val=0.3615483343601227)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0175], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.805467128753662, max_val=1.6490662097930908)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_0): ConvBn2d(\n",
       "    144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.835041880607605, max_val=0.8234864473342896)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0848], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.352572441101074, max_val=10.260930061340332)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_0_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.750647246837616, max_val=0.6420109868049622)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0873], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.293466567993164, max_val=10.977378845214844)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_0_1): Identity()\n",
       "  (net_encoder_layer2_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0439], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2800154387950897, max_val=10.919029235839844)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0112], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4182199239730835, max_val=1.4288922548294067)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1004], device='cuda:0'), zero_point=tensor([117], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.74235725402832, max_val=13.856621742248535)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_1_1): Identity()\n",
       "  (net_encoder_layer2_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0549], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2791571617126465, max_val=13.730056762695312)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer2_1_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0139], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4055018424987793, max_val=1.7671310901641846)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0447], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.906805515289307, max_val=5.486788749694824)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0222], device='cuda:0'), zero_point=tensor([13], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.28020039200782776, max_val=5.379629135131836)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0034], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3406764566898346, max_val=0.4319320321083069)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0172], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2264726161956787, max_val=2.1635994911193848)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_0): ConvBn2d(\n",
       "    240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0071], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9003563523292542, max_val=0.6918697953224182)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1000], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.192778587341309, max_val=12.316064834594727)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer2_1_block_3_1): Identity()\n",
       "  (net_encoder_layer2_1_stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "  (net_encoder_layer3_0_block_0_0): ConvBn2d(\n",
       "    40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4676007926464081, max_val=0.4575655460357666)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1002], device='cuda:0'), zero_point=tensor([124], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.412806510925293, max_val=13.140999794006348)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_0_1): Identity()\n",
       "  (net_encoder_layer3_0_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0524], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2790752947330475, max_val=13.083654403686523)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_0): ConvBn2d(\n",
       "    240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False\n",
       "    (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0224], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5601437091827393, max_val=2.8545138835906982)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0968], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.57463550567627, max_val=13.11510181427002)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_1_1): Identity()\n",
       "  (net_encoder_layer3_0_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0521], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2784714102745056, max_val=13.002901077270508)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_0_block_2_fc1): Conv2d(\n",
       "    240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0121], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.543447494506836, max_val=1.4003231525421143)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0379], device='cuda:0'), zero_point=tensor([92], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4827473163604736, max_val=6.180971622467041)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0234], device='cuda:0'), zero_point=tensor([12], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2797144949436188, max_val=5.6879730224609375)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_fc2): Conv2d(\n",
       "    10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.42404410243034363, max_val=0.3145257830619812)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0219], device='cuda:0'), zero_point=tensor([133], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.9138174057006836, max_val=2.6629021167755127)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_0): ConvBn2d(\n",
       "    240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0073], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9153355956077576, max_val=0.9257211089134216)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0873], device='cuda:0'), zero_point=tensor([136], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.863374710083008, max_val=10.400520324707031)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_0_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6364786624908447, max_val=0.5366501212120056)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0872], device='cuda:0'), zero_point=tensor([121], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.527286529541016, max_val=11.70177173614502)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_0_1): Identity()\n",
       "  (net_encoder_layer3_1_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0468], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2798554599285126, max_val=11.642064094543457)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0265], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.381244421005249, max_val=2.4333271980285645)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1124], device='cuda:0'), zero_point=tensor([121], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.570856094360352, max_val=15.088375091552734)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_1_1): Identity()\n",
       "  (net_encoder_layer3_1_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0597], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.27982524037361145, max_val=14.936224937438965)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_1_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0107], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.368058204650879, max_val=1.2073206901550293)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0551], device='cuda:0'), zero_point=tensor([119], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.525923728942871, max_val=7.513123989105225)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0303], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2791365683078766, max_val=7.436128616333008)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4135446846485138, max_val=0.37192434072494507)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0428], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.061733245849609, max_val=5.859547138214111)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8316009044647217, max_val=0.7828212380409241)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1016], device='cuda:0'), zero_point=tensor([115], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.732812881469727, max_val=14.175081253051758)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_1_block_3_1): Identity()\n",
       "  (net_encoder_layer3_1_stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
       "  (net_encoder_layer3_2_block_0_0): ConvBn2d(\n",
       "    80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35769110918045044, max_val=0.4149148762226105)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1006], device='cuda:0'), zero_point=tensor([124], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.505239486694336, max_val=13.142346382141113)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_0_1): Identity()\n",
       "  (net_encoder_layer3_2_block_0_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0522], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2796258330345154, max_val=13.03331184387207)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_0): ConvBn2d(\n",
       "    480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False\n",
       "    (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0359], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.582597732543945, max_val=3.159688949584961)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1161], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.976546287536621, max_val=15.619869232177734)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_1_1): Identity()\n",
       "  (net_encoder_layer3_2_block_1_2): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0620], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2794046998023987, max_val=15.535189628601074)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (net_encoder_layer3_2_block_2_fc1): Conv2d(\n",
       "    480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0104], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3258647918701172, max_val=1.2849180698394775)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0496], device='cuda:0'), zero_point=tensor([109], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.40542459487915, max_val=7.235474109649658)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_activation): QSiLU(\n",
       "    (act): Sigmoid(\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "    (f_mul): FloatFunctional(\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0274], device='cuda:0'), zero_point=tensor([10], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2799387574195862, max_val=6.7173662185668945)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_fc2): Conv2d(\n",
       "    20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4529843032360077, max_val=0.48793476819992065)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0375], device='cuda:0'), zero_point=tensor([116], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.369065284729004, max_val=5.205258369445801)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_2_scale_activation): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_0): ConvBn2d(\n",
       "    480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0067], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.810765266418457, max_val=0.8555827140808105)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1061], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.539660453796387, max_val=13.508418083190918)\n",
       "    )\n",
       "  )\n",
       "  (net_encoder_layer3_2_block_3_1): Identity()\n",
       "  (net_encoder_layer3_2_stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_0): ConvBnReLU2d(\n",
       "    24, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0023], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.297598272562027, max_val=0.26047536730766296)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0567], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.449585914611816)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_0_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_0): ConvBnReLU2d(\n",
       "    40, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0021], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2729571759700775, max_val=0.2730327248573303)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0562], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.338400840759277)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_1_layers_2): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_0): ConvBnReLU2d(\n",
       "    80, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0015], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.17631450295448303, max_val=0.18941503763198853)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0582], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=14.832235336303711)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_1): Identity()\n",
       "  (net_decoder_layers_0_adapter_conv_2_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0007], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07744815945625305, max_val=0.09268926084041595)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_0): ConvBnReLU2d(\n",
       "    256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0018], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2091578096151352, max_val=0.23418985307216644)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0636], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=16.206953048706055)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_32_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_32_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_0): ConvBnReLU2d(\n",
       "    128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0004], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.03609912097454071, max_val=0.05075689032673836)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_sm_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_0): ConvBnReLU2d(\n",
       "    128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0016], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.1917170137166977, max_val=0.2031281590461731)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=17.617536544799805)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_16_up_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_16_up_layers_2): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_0): ConvBnReLU2d(\n",
       "    64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0005], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.07010912150144577, max_val=0.06318911910057068)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_1): Identity()\n",
       "  (net_decoder_layers_1_fam_8_sm_layers_2): Identity()\n",
       "  (net_final_conv): Conv2d(\n",
       "    384, 1, kernel_size=(3, 3), stride=(1, 1), padding=same\n",
       "    (weight_fake_quant): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0002], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.020986726507544518, max_val=0.027254510670900345)\n",
       "    )\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.010998968034982681, max_val=1.0446158647537231)\n",
       "    )\n",
       "  )\n",
       "  (fake_dequant_0): DeQuantStub()\n",
       "  (float_functional_simple_0): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0253], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.21868912875652313, max_val=6.226154327392578)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_1): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0325], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2288961112499237, max_val=8.05055046081543)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_2): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0369], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.223818838596344, max_val=9.18370246887207)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_3): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1421], device='cuda:0'), zero_point=tensor([101], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.349632263183594, max_val=21.89630699157715)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_4): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0275], device='cuda:0'), zero_point=tensor([9], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23703870177268982, max_val=6.766177177429199)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_5): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0395], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24966809153556824, max_val=9.833389282226562)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_6): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1682], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-22.456207275390625, max_val=20.423616409301758)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_7): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0314], device='cuda:0'), zero_point=tensor([8], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.24751269817352295, max_val=7.767127513885498)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_8): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0404], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.30226606130599976, max_val=9.988883018493652)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_9): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1707], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-21.61538314819336, max_val=21.90613555908203)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_10): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0451], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.25835850834846497, max_val=11.240936279296875)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_11): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2577], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-32.49716567993164, max_val=33.210166931152344)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_12): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=26.71375274658203)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_13): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1005], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=25.63890266418457)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_14): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       "  (float_functional_simple_15): FloatFunctional(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=15.582015037536621)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate the model with quantization error via fake quantization\n",
    "qat_model.apply(torch.quantization.disable_observer)\n",
    "# validate here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
